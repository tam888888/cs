<!doctype html><html lang="en" class="no-js"><head><meta charset="utf-8"> <!-- begin SEO --><title>Welcome - Olivia Y. Lee</title><meta property="og:locale" content="en-US"><meta property="og:site_name" content="Olivia Y. Lee"><meta property="og:title" content="Welcome"><link rel="canonical" href="https://oliviaylee.github.io/"><meta property="og:url" content="https://oliviaylee.github.io/"><meta property="og:description" content="About me"> <script type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : "Olivia Y. Lee", "url" : "https://oliviaylee.github.io", "sameAs" : null } </script> <!-- end SEO --><link href="https://oliviaylee.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Olivia Y. Lee Feed"> <!-- http://t.co/dKP3o1e --><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width, initial-scale=1.0"> <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script> <!-- For all browsers --><link rel="stylesheet" href="https://oliviaylee.github.io/assets/css/main.css"><meta http-equiv="cleartype" content="on"> <!-- start custom head snippets --><link rel="apple-touch-icon" sizes="57x57" href="https://oliviaylee.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="60x60" href="https://oliviaylee.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="72x72" href="https://oliviaylee.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="76x76" href="https://oliviaylee.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="114x114" href="https://oliviaylee.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="120x120" href="https://oliviaylee.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="144x144" href="https://oliviaylee.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="152x152" href="https://oliviaylee.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ"><link rel="apple-touch-icon" sizes="180x180" href="https://oliviaylee.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ"><link rel="icon" type="image/png" href="https://oliviaylee.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32"><link rel="icon" type="image/png" href="https://oliviaylee.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192"><link rel="icon" type="image/png" href="https://oliviaylee.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96"><link rel="icon" type="image/png" href="https://oliviaylee.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16"><link rel="manifest" href="https://oliviaylee.github.io/images/manifest.json?v=M44lzPylqQ"><link rel="mask-icon" href="https://oliviaylee.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000"><link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ"><meta name="msapplication-TileColor" content="#000000"><meta name="msapplication-TileImage" content="https://oliviaylee.github.io/images/mstile-144x144.png?v=M44lzPylqQ"><meta name="msapplication-config" content="https://oliviaylee.github.io/images/browserconfig.xml?v=M44lzPylqQ"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://oliviaylee.github.io/assets/css/academicons.css"/> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script> <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script> <!-- end custom head snippets --></head><body> <!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]--><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav"> <button><div class="navicon"></div></button><ul class="visible-links"><li class="masthead__menu-item masthead__menu-item--lg"><a href="https://oliviaylee.github.io/">Olivia Y. Lee</a></li><li class="masthead__menu-item"><a href="https://oliviaylee.github.io/research/">Research</a></li><li class="masthead__menu-item"><a href="https://oliviaylee.github.io/portfolio/">Portfolio</a></li><li class="masthead__menu-item"><a href="https://oliviaylee.github.io/files/cv.pdf">CV</a></li></ul><ul class="hidden-links hidden"></ul></nav></div></div></div><div id="main" role="main"><div class="sidebar sticky"><div itemscope itemtype="http://schema.org/Person"><div class="author__avatar"> <img src="https://oliviaylee.github.io/images/profile.png" class="author__avatar" alt="Olivia Y. Lee"></div><div class="author__content"><h3 class="author__name">Olivia Y. Lee</h3><p class="author__bio">B.S. Symbolic Systems + Math, M.S. Computer Science</p></div><div class="author__urls-wrapper"> <button class="btn btn--inverse">Follow</button><ul class="author__urls social-icons"><li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Stanford University</li><li><a href="mailto:oliviayl@stanford.edu"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li><li><a href="https://twitter.com/olivia_y_lee"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li><li><a href="https://www.linkedin.com/in/oliviaylee"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li><li><a href="https://github.com/oliviaylee"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li><li><a href="https://scholar.google.com/citations?hl=en&user=Ku0IK-UAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li></ul></div></div></div><article class="page" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="headline" content="Welcome"><meta itemprop="description" content="About me"><div class="page__inner-wrap"><header><h1 class="page__title" itemprop="headline">Welcome</h1></header><section class="page__content" itemprop="text"><p>Hello! I am a final year undergraduate student at Stanford University (Class of 2024), pursuing a B.S. in <a href="https://symsys.stanford.edu/">Symbolic Systems</a> with a minor in Mathematics and a coterminal M.S. in Computer Science. I conduct research with Stanford’s <a href="https://irislab.stanford.edu/">IRIS Lab</a> which studies intelligence through robotic interaction at scale, affiliated with the <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory (SAIL)</a> and <a href="http://ml.stanford.edu/index.html">Stanford Machine Learning Group</a>. I am fortunate to be mentored by Professor <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://cs.stanford.edu/~surajn/">Suraj Nair</a>, and <a href="https://anxie.github.io/">Annie Xie</a>.</p><p>My research interests span robotics, machine learning, and computer vision. I’m interested in enabling robots to <em>learn generalizable representations from diverse datasets</em>, and <em>refine them through interaction</em> for performing complex tasks in the real world. I’m especially interested in:</p><ol><li><strong>Visual pretraining and representation learning</strong>: Enabling robots to harness skill and object representations for downstram tasks, potentially leveraging large pretrained models.</li><li><strong>Interactive learning from multimodal data</strong>: Facilitating human-compatible robot behaviors by enabling robots to process multimodal inputs and feedback.</li><li><strong>Continual data collection and learning</strong>: Improving methods for continually acquiring in-domain data and skills with limited supervision for novel environment adaptation.</li></ol><p>Inspired by my interdisciplinary coursework, I am drawn to research leveraging concepts in cognitive science for robot learning and visual understanding. I aim to better understand human cognitive processes, such as multimodal perception, curiosity, and interactive learning, to develop human-inspired learning algorithms for robotics.</p><p>If any of the above sounds interesting to you, I would love to hear from you! Feel free to reach me at oliviayl [at] stanford [dot] edu.</p><!-- 4/1/2024: 1. Visual pretraining and representation learning: I am excited by the potential of embodied agents learning skill and object representations via pretraining on large, diverse datasets, and using them for sample-efficient exploration or downstream tasks. 2. Interactive learning from multimodal human data: Humans communicate goals using various modalities, from language to physical corrections. I hope to facilitate human-compatible robot behaviors by enabling robots to process multimodal inputs and feedback, potentially leveraging large pretrained models. 3. Continual data collection and learning: Ideally robots should continually acquire experience and skills with limited supervision. I aim to improve autonomous exploration methods for scalably collecting in-domain robot data and adapting to novel environments.--> <!-- 12/6/2023(2): 1. **Visual pretraining and representation learning**: In novel situations, humans don't re-learn skills and object representations from scratch. I am excited by the potential of robotic agents similarly learning environmental representations via pretraining on large, diverse datasets, and using these representations for exploration or downstream tasks. 2. **Interactive learning from multimodal human data**: Humans communicate goals and provide feedback using various modalities, from language to physical corrections. I hope to develop methods capable of understanding multimodal task specifications to generate more expressive, human-compatible behaviors in robots, potentially leveraging Internet-scale multimodal pretrained models. 3. **Continual data collection and learning**: Supervision is costly, and I am excited by the prospect of robots continually acquiring knowledge and skills with limited supervision, as humans often do. Autonomous exploration also facilitates scalable, in-domain robot data collection, and I aim to improve learning from exploration for adaptively performing downstream tasks in novel environments. --> <!-- 12/6/2023: More broadly, I'm interested in *embodied systems capable of intelligently exploring their environments, and harnessing learned knowledge for downstream tasks*. When faced with novel situations, humans don't re-learn skills and object representations from scratch. I am excited by the potential of robotic agents similarly learning representations of the environment, for instance via pretraining on (potentially multimodal) information, and using these representations to explore novel environments intelligently through interaction. After familiarizing itself with the specific objects and dynamics the new environment, the robot can then proceed with its assigned tasks. ... Through my current coursework and research, ... Some areas of human cognition that I hope to explore through a computational lens are multimodal perception, interactive learning, and curiosity.... engineer computational analogs of these processes in AI systems.--> <!-- OLD: More broadly, I'm interested in embodied intelligent systems capable of learning quickly and flexibly by cooperating with humans. I am excited by the interplay between autonomous and interactive reinforcement learning: a robot should ideally operate and learn autonomously, but query a human operator upon recognizing it has reached an irreversible or unsafe state. By processing human information and feedback, potentially from multiple modalities (language, images, physical repositioning etc.), the robot can then proceed with its assigned tasks. --></section><footer class="page__meta"></footer></div></article></div><div class="page__footer"><footer> <!-- start custom footer snippets --> <a href="/sitemap/">Sitemap</a> <!-- end custom footer snippets --><div class="page__footer-follow"><ul class="social-icons"><li><strong>Follow:</strong></li><li><a href="http://github.com/oliviaylee"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li><li><a href="https://oliviaylee.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li></ul></div><div class="page__footer-copyright">&copy; 2024 Olivia Y. Lee. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div></footer></div><script src="https://oliviaylee.github.io/assets/js/main.min.js"></script> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script></body></html>
