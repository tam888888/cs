<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>On Chomsky and the Two Cultures of Statistical Learning</title>
</head>

<body  style="max-width: 42em">
<h1>On Chomsky and the Two Cultures of Statistical Learning</h1>

At the  <a
href="http://mit150.mit.edu/symposia/brains-minds-machines">Brains,
Minds, and Machines</a> symposium held during MIT's 150th birthday party in 2011, Technology Review
<a
href="http://www.technologyreview.com/computing/37525/?a=f">reports</a>
that Prof. Noam Chomsky

<div style="float:right; margin:8px; text-align:center">
  <a href="http://mit150.mit.edu/symposia/brains-minds-machines.html"><img
  src="https://mit150.mit.edu/images/symposia-graphic6.jpg"
  title="MIT:Brains, Minds, and Machines" width=106></a>
  <br>MIT: 150
</div>

<blockquote>
derided researchers in machine learning who use purely
statistical methods to produce behavior that mimics something in the
world, but who don't try to understand the meaning of that
behavior. 
</blockquote>

The <a href="http://languagelog.ldc.upenn.edu/myl/PinkerChomskyMIT.html">transcript</a> is now available, so let's quote 
Chomsky himself:

<blockquote>
It's true there's been a lot of work on trying to apply statistical models to
various linguistic problems. I think there have been some successes, but a lot of failures.
There is a notion of success ... which I think is novel in the history of science.
It interprets success as approximating unanalyzed data.
</blockquote>


<div style="float:right; margin:8px; text-align:center">
  <a href="https://en.wikipedia.org/wiki/Noam_Chomsky"><img
  src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Noam_Chomsky_portrait_2017_retouched.png/440px-Noam_Chomsky_portrait_2017_retouched.png"
  title="Noam Chomsky" width=106></a>
  <bR>Noam Chomsky
</div>


<p>Chomsky's remarks were in response to Steven Pinker's question
about the success of probabilistic models trained with statistical methods.
This essay is a response to Pinker and Chomsky, and will address these questions:


<ol>
<li>What did Chomsky mean, and is he right?
<li>What is a statistical model?
<li>How successful are statistical language models?
<li>Is there anything like their notion of success in the history of
  science?
<li>What doesn't Chomsky like about statistical models?
</ol>

<h2>What did Chomsky mean, and is he right?</h2>

I take Chomsky's points to be the following:
<ol type=A>
<li>Statistical language models have had engineering success, but that
  is irrelevant to science.
<li>Accurately modeling linguistic facts is just butterfly collecting;
  what matters in science (and specifically linguistics) is the underlying principles.
<li>Statistical models are incomprehensible; they provide no insight.
<li>Statistical models may provide an accurate simulation of some
  phenomena, but the simulation is done completely the wrong way; people don't
  decide what the third word of a sentence should be by consulting a
  probability table keyed on the previous words, rather they map
  from an internal semantic form to a syntactic tree-structure, which
  is then linearized into words. This is done without any probability
  or statistics.
<li>Statistical models have been proven incapable of learning
  language; therefore language must be innate, so why are these
  statistical modelers wasting their time on the wrong enterprise?
</ol>

Is he right? That's a long-standing debate. These are my short answers:
<ol type=A>
<li>I agree that engineering success is not the sole goal or the measure of
  science. But I observe that science and engineering develop together,
  and that engineering success shows that something is working right,
  and so is evidence (but not proof) of a scientifically successful model.
<li>Science is a combination of gathering facts and making theories;
  neither can progress on its own. In the history of
  science, the laborious accumulation of
  facts is the dominant mode, not a novelty. The science of understanding language is no different than
  other sciences in this respect.
<li>I agree that it can be difficult to make sense of a model
  containing billions of parameters.  Certainly a human can't understand
  such a model by inspecting the values of each parameter individually.  But one
  can gain insight by examing the <i>properties</i> of the model&mdash;where
  it succeeds and fails, how well it learns as a function of data, etc.
<li>I agree that a Markov model of word probabilities cannot model all
  of language.  It is equally true that a concise tree-structure model without
  probabilities cannot model all of language. What is needed is a
  probabilistic model that covers words, syntax, semantics, context,
  discourse, etc. Chomsky dismisses all probabilistic models because
  of shortcomings of a particular 50-year old probabilistic model.
  I understand how Chomsky might arrive at the conclusion that
  probabilistic models are unnecessary, from his
  study of the <i>generation</i> of language.  But the vast majority of people who study
  <i>interpretation</i> tasks (such as speech recognition) quickly see that
  interpretation is an inherently probabilistic problem: given a
  stream of noisy input to
  my ears, what did the speaker most likely mean? Einstein said to
  make  everything as simple as possible, but no
  simpler. Many phenomena in science are stochastic, and the simplest
  model of them is a probabilistic model; I believe language is such a
  phenomenon and therefore that probabilistic models are our best tool
  for representing facts about language, for algorithmically
  processing language, and for understanding how humans process language.
<li>In 1967, Gold's Theorem showed some theoretical limitations of logical
  deduction on formal mathematical languages. But this result has
  nothing to do with the task faced by learners of natural language.
  In any event, by 1969 we knew that probabilistic inference
  (over probabilistic context-free grammars) is not subject to those limitations (Horning showed that learning of PCFGs is possible).
  I agree with Chomsky that it is undeniable that humans
  have some innate capability to learn natural language, but we don't
  know enough about that capability to say how it works; it certainly could
  use something like probabilistic language representations and
  statistical learning. And we don't know if the innate ability is specific to language, or
  is part of a more general ability that works for language and other things.
</ol>

The rest of this essay consists of longer versions of each answer.

<h2>What is a statistical model?</h2>

A <b>statistical model</b> is a mathematical model which is modified
or trained by the input of data points. Statistical models are often
but not always probabilistic. Where the
distinction is important we will be careful not to just say
"statistical" but to use the following component terms:
<ul>
  <li> A <b>mathematical model</b> specifies a relation among
  variables, either in functional form that maps inputs
  to outputs (e.g. <i>y</i> = <i>m</i> <i>x</i> + <i>b</i>) or in 
  relation form (e.g. the following (<i>x</i>, <i>y</i>) pairs are
  part of the relation).

  <li> A <b>probabilistic model</b>  specifies a
probability distribution over possible values of random variables,
  e.g., P(<i>x</i>, <i>y</i>),
rather than a strict deterministic relationship, e.g., <i>y</i> = <i>f</i>(<i>x</i>).

 
  <li> A <b>trained model</b> uses some training/learning algorithm to
  take as input a collection of possible models and a collection of
  data points (e.g. (<i>x</i>,
  <i>y</i>) pairs) and select the best model.  Often this is in the form of
  choosing the values of parameters (such as
  <i>m</i> and <i>b</i> above) through a process of
  statistical inference.
</ul>


<div style="float:right; margin:8px; text-align:center">
  <a href="http://en.wikipedia.org/wiki/Claude_Shannon"><img
  src="https://upload.wikimedia.org/wikipedia/commons/9/99/ClaudeShannon_MFO3807.jpg"
  title="Claude Shannon" width=106></a>
  <br>Claude Shannon
</div>

<p>For example, a decade before Chomsky, Claude Shannon <a
href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">proposed probabilistic models of 
communication</a>  based on Markov chains of words. If you have
a vocabulary of 100,000 words and a second-order Markov model in which the probability
of a word depends on the previous two words, then you need a quadrillion (10<sup>15</sup>)
probability values to specify the model. The only feasible way to learn
these 10<sup>15</sup> values is to gather statistics from data and introduce
some smoothing method for the many cases where there is no
data. Therefore, most (but not all) probabilistic models are
trained. Also, many (but not all) trained models are probabilistic.

<p>As another example, consider the Newtonian model of gravitational
attraction, which says that the force between two objects of mass
<i>m</i><sub>1</sub> and <i>m</i><sub>2</sub> a distance <i>r</i> apart
is given by

<blockquote>
<i>F</i> = <i>G</i> <i>m</i><sub>1</sub> <i>m</i><sub>2</sub> / <i>r</i><sup>2</sup>
</blockquote>

where <i>G</i> is the universal gravitational constant.  This is a
trained model because the gravitational constant <i>G</i> is
determined by statistical inference over the results of a series of
experiments that contain stochastic experimental error. It is also a
deterministic (non-probabilistic) model because it states an exact
functional relationship. 
I believe that Chomsky has no objection to
this kind of statistical model. Rather, he seems to reserve his
criticism for statistical models like Shannon's that have quadrillions of
parameters, not just one or two.


<p>(This example brings up another distinction:
the gravitational model is <b>continuous</b> and <b>quantitative</b>
whereas the linguistic tradition has favored models that are
<b>discrete</b>, <b>categorical</b>, and <b>qualitative</b>: a word is or is not a verb,
there is no question of its degree of verbiness. For more on these
distinctions, see Chris Manning's article on <a
href="http://nlp.stanford.edu/~manning/papers/probsyntax.pdf">Probabilistic
Syntax</a>.)

<p>A relevant probabilistic statistical model is the <a
href="http://en.wikipedia.org/wiki/Ideal_gas_law">ideal gas law</a>,
which describes the pressure <i>P</i> of a gas in terms of the the
number of molecules, <i>N</i>, the temperature <i>T</i>, and
Boltzmann's constant, <i>K</i>:
<blockquote>
<i>P</i> = <i>N k T / V</i>.
</blockquote>


<p>The equation can be derived from first principles
using the tools of statistical mechanics.  It is an uncertain,
incorrect model; the <i>true</i> model would have to describe the
motions of individual gas molecules. This model ignores that
complexity and <i>summarizes</i> our uncertainty about the location of
individual molecules.  Thus, even though it is statistical and
probabilistic, even though it does not completely model reality,
it does provide both good predictions and insight&mdash;insight that
is not available from trying to understand the <i>true</i> movements
of individual molecules.

<p>Now let's consider the non-statistical
model of spelling expressed by the rule "<i>I before E except after C.</i>"
Compare that to the probabilistic, trained statistical model:
<blockquote>
<pre>
P(IE) = 0.0177         P(CIE) = 0.0014        P(*IE) = 0.163
P(EI) = 0.0046         P(CEI) = 0.0005        P(*EI) = 0.0041
</pre>
</blockquote>
This model comes from statistics on a <a href="http://norvig.com/ngrams/">corpus of a trillion words</a> of English
text. The notation P(IE) is the probability that a word sampled from
this corpus contains the consecutive letters "IE." P(CIE) is the probability that a word contains the consecutive letters "CIE", and P(*IE) is the probability of any letter other than C followed by IE.
The statistical
data confirms that IE is in fact more common than EI (by almost 4 to 1), and that the dominance
of IE lessens when following a C, but contrary to the rule, CIE is still more common than CEI, by almost 3 to 1. Examples of "CIE" words
include "science," "society," "ancient" and "species." 
The disadvantage of the "I
before E except after C" model is that it is not very accurate. Consider:
<pre>
Accuracy("I before E") = 0.0177/(0.0177+0.0046) = 0.793
Accuracy("I before E except after C") = (0.0005+0.0163)/(0.0005+0.0163+0.0014+0.0041) = 0.753
</pre>
A more complex statistical model (say, one that gave the probability of all
4-letter sequences, and/or of all known words) could be <a href="http://norvig.com/spell-correct.html">ten times more
accurate</a> at the task of spelling, but does not offer concise <b>insight</b> into how spelling
works. Insight would require a model that knows about phonemes,
syllabification, and language of origin.  Such a model could be trained (or not) and
probabilistic (or not).


<p>As another example of insight,
consider the Theory of Supreme Court Justice Hand-Shaking: when the
supreme court convenes, all attending justices shake hands with every
other justice.  The number of attendees, <i>n</i>, must be an integer
in the range 0 to 9; what is the total number of handshakes, <i>h</i>
for a given <i>n</i>? Here are three possible explanations:

<ol type=A>
<li>Each of <i>n</i> justices shakes hands with the other
<i>n</i> - 1 justices, but that counts Alito/Breyer and Breyer/Alito as
two separate shakes, so we should cut the total in half, and we end up
with <i>h</i> = <i>n</i> &times; (<i>n</i> - 1) / 2.

<li>To avoid double-counting, we will order the justices by
seniority and only count a more-senior/more-junior handshake, not a
more-junior/more-senior one.  So we count, for each justice, the
shakes with the more junior justices, and sum them up, giving <i>h</i>
= <font size="+1">&Sigma;</font><sub><i>i</i> = 1 .. <i>n</i></sub>
(<i>i</i> - 1).

<li>Just look at this table:

<table border=1 cellspacing=0 cellpadding=3>
<tr><td><i>n</i>:<td>0<td>1 <td>2 <td>3 <td>4 <td>5 <td>6 <td>7 <td>8 <td>9
<tr><td><i>h</i>: <td>0 <td>0 <td>1 <td>3 <td>6 <td>10 <td>15 <td>21
<td>28 <td>36
  </table>
</ol>

Some people might prefer A, some might prefer B, and if you are slow
at doing multiplication or addition you might prefer C.
Why? All three explanations describe <i>exactly the same theory</i> &mdash;
the same function from <i>n</i> to <i>h</i>, over the entire domain of
possible values of <i>n</i> (as long as there are no  more than 9 supreme court justices).  Thus we could prefer A (or B) over C
only for reasons other than the theory itself.  We might find that A
or B gave us a better understanding of the problem.  A and B are
certainly more useful than C for figuring out what happens if Congress
exercises its power to add an additional justice. Theory A
might be most helpful in developing a theory of handshakes at the end
of a hockey game (when each player shakes hands with players on the
opposing team) or in proving that the number of people who shook an
odd number of hands at the MIT Symposium is even.

<h2>How successful are statistical language models?</h2>

Chomsky said words to the effect that statistical language models have
had some limited success
in some application areas. Let's look
at computer systems that deal with language, and at the notion of
"success" defined by "making accurate predictions about the world."  First, the
major application areas:

<ul> <li><b>Search engines:</b> 100% of major players are trained
  and probabilistic. Their operation cannot be described by a simple function.

  <li><b>Speech recognition:</b> 100% of major systems are
  trained and probabilistic, mostly relying on probabilistic
  hidden Markov models.

  <li><b>Machine translation:</b> 100% of top competitors in
  competitions such as <a
  href="http://www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease/currentArabic.html">NIST</a>
  use statistical methods.  Some commercial systems use a hybrid of
  trained and rule-based approaches. Of the 4000 language pairs
  covered by machine translation systems, a statistical system is by
  far the best for every pair except Japanese-English, where the top
  statistical system is roughly equal to the top hybrid system.

  <li><b>Question answering:</b> this application is less
  well-developed, and many systems build heavily on the statistical
  and probabilistic approach used by search engines. The <a
  href="http://www-03.ibm.com/innovation/us/watson/index.html">IBM
  Watson</a> system that recently won on Jeopardy is thoroughly
  probabilistic and trained, while Boris Katz's <a
  href="http://groups.csail.mit.edu/infolab/publications/Katz-etal-TREC2003.pdf">START</a>
  is a hybrid. All systems use at least some statistical techniques.
  </ul>

  <p>Now let's look at some components that are of interest only to
  the computational linguist, not to the end user:
  
  <ul>

  <li><b>Word sense disambiguation:</b> 100% of top competitors at the
  <a
  href="http://semeval2.fbk.eu/semeval2.php?location=SemEval2010-Program">SemEval-2</a>
  competition used statistical techniques; most are probabilistic; some use a hybrid approach
  incorporating rules from sources such as Wordnet.

  <li><b>Coreference resolution:</b> The majority of current systems
  are statistical, although we should mention the system of <a
  href="http://www.aclweb.org/anthology/D/D09/D09-1120.pdf">Haghighi
  and Klein</a>, which can be described as a hybrid system that is
  mostly rule-based rather than trained, and performs on par with
  top statistical systems.

  <li><b>Part of speech tagging:</b> Most current systems are
  statistical. The <a href="http://en.wikipedia.org/wiki/Brill_tagger">Brill tagger</a> stands out as a successful hybrid
  system: it learns a set of deterministic rules from statistical
  data.

  <li><b>Parsing:</b> There are many parsing systems, using multiple
    approaches. Almost all of the <a
    href="http://csep.psyc.memphis.edu/pdf/Evaluating_treebank_Parsers_Coh-Metrix.pdf">most
    successful</a> are statistical, and the majority are <a
    href="http://www.cs.berkeley.edu/~pliang/papers/hdppcfg-haba.pdf">probabilistic</a>
    (with a substantial minority of deterministic parsers).
</ul>

<p>Clearly, it is inaccurate to say that statistical models
(and probabilistic models) have achieved <i>limited</i> success; rather they
have achieved an <i>overwhelmingly dominant</i> (although not exclusive) position.

<p>Another measure of success is the degree to which an idea captures
a community of researchers.  As Steve Abney <a href="http://www.vinartus.net/spa/95c.pdf">wrote</a> in 1996, "In the
space of the last ten years, statistical methods have gone from being
virtually unknown in computational linguistics to being a fundamental
given. ... anyone who cannot at least use the terminology persuasively
risks being mistaken for kitchen help at the ACL [Association for
Computational Linguistics] banquet."

<p>Now of course, the majority doesn't rule -- just because everyone is jumping on some bandwagon, that doesn't make it right.  
But I made the switch: after about 14 years of trying to get language models to work using logical rules, 
I started to adopt probabilistic approaches (thanks to pioneers like Gene Charniak (and Judea Pearl for probability in general) 
and to my colleagues who were early adopters, like Dekai Wu).  
And I saw everyone around me making the same switch.  
And I didn't see anyone going in the other direction.  We all saw the limitations of the old tools, and the benefits of the new.

<p>And while it may seem crass and anti-intellectual to consider a
financial measure of success, it is worth noting that the intellectual
<a href="http://en.wikipedia.org/wiki/Telecommunication">offspring</a>
of Shannon's theory create several trillion dollars of revenue each
year, while the <a
 href="http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=chomsky&x=0&y=0">offspring</a>
of Chomsky's theories generate well under a billion.

<p>This section has shown that one reason why the vast majority of
researchers in computational linguistics use statistical models is an
<i>engineering</i> reason: statistical models have state-of-the-art
performance, and in most cases non-statistical models perform worst.
For the remainder of this essay we will concentrate on
<i>scientific</i> reasons: that probabilistic models better represent
linguistic facts, and statistical techniques make it easier for us to
make sense of those facts.

<h2>Is there anything like it [the statistical notion of success] in the history of science?</h2>

When Chomsky said "<i>That's a
notion of [scientific] success that's very novel. I don't know of
anything like it in the history of science</i>"  he apparently meant
that the notion of success of "accurately modeling the world" is
novel, and that the only true measure of success in the history of science
is "providing insight" &mdash; of answering <i>why</i> things are the way
they are, not just describing <i>how</i> they are.

<p>A <a href="http://www.google.com/webhp?sourceid=chrome-instant&ie=UTF-8&ion=1&nord=1#sclient=psy&hl=en&tbo=1&nord=1&tbs=dfn:1&source=hp&q=science&aq=f&aqi=p-p2g-e2g-c2g2g-c1g1&aql=&oq=&pbx=1&tbo=1&bav=on.2,or.r_gc.r_pw.&fp=18c4c2d0f0f5fea9&biw=1186&bih=634">dictionary definition</a> of science is "the systematic study of the
structure and behavior of the physical and natural world through
observation and experiment," which stresses accurate modeling over insight, but it
seems to me that both notions have always coexisted as part of doing
science. To test that, I consulted the epitome of doing science,
namely <i><a href="http://www.sciencemag.org">Science</a></i>.  I
looked at the current issue and chose a title and abstract at random:


<div style="float:right; margin:8px">
  <a href="http://www.sciencemag.org/content/332/6032.toc"><img
  src="https://www.science.org/cms/asset/fb775384-dba8-4173-826f-7cc4af196904/science.2011.332.issue-6032.largecover.jpg"
  title="Science" width=106></a>
</div>

<blockquote>
<a href="http://www.sciencemag.org/content/332/6032/944.abstract">Chlorinated Indium Tin Oxide Electrodes with High Work Function for
Organic Device Compatibility</a>

<p>In organic light-emitting diodes (OLEDs), a stack of multiple organic
layers facilitates charge flow from the low work function [~4.7
electron volts (eV)] of the transparent electrode (tin-doped indium
oxide, ITO) to the deep energy levels (~6 eV) of the active
light-emitting organic materials. We demonstrate a chlorinated ITO
transparent electrode with a work function of >6.1 eV that provides a
direct match to the energy levels of the active light-emitting
materials in state-of-the art OLEDs. A highly simplified green OLED
with a maximum external quantum efficiency (EQE) of 54% and power
efficiency of 230 lumens per watt using outcoupling enhancement was
demonstrated, as were EQE of 50% and power efficiency of 110 lumens
per watt at 10,000 candelas per square meter.
</blockquote>

<p>It certainly seems that this article is much more focused on
"accurately modeling the world" than on "providing insight."  The
paper does indeed fit in to a body of theories, but it is mostly
reporting on specific experiments and the results obtained from them
(e.g. efficiency of 54%).

<p>I then looked at all the titles and abstracts from the <a href="http://www.sciencemag.org/content/332/6032.toc">current
issue</a> of <i>Science</i>:

<ul>
<li>Comparative Functional Genomics of the Fission Yeasts
<li>Dimensionality Control of Electronic Phase Transitions in Nickel-Oxide Superlattices
<li>Competition of Superconducting Phenomena and Kondo Screening at the Nanoscale
<li>Chlorinated Indium Tin Oxide Electrodes with High Work Function for Organic Device Compatibility
<li>Probing Asthenospheric Density, Temperature, and Elastic Moduli Below the Western United States
<li>Impact of Polar Ozone Depletion on Subtropical Precipitation
<li>Fossil Evidence on Origin of the Mammalian Brain
<li>Industrial Melanism in British Peppered Moths Has a Singular and Recent Mutational Origin
<li>The Selaginella Genome Identifies Genetic Changes Associated with the Evolution of Vascular Plants
<li>Chromatin "Prepattern" and Histone Modifiers in a Fate Choice for Liver and Pancreas
<li>Spatial Coupling of mTOR and Autophagy Augments Secretory Phenotypes
<li>Diet Drives Convergence in Gut Microbiome Functions Across Mammalian Phylogeny and Within Humans
<li>The Toll-Like Receptor 2 Pathway Establishes Colonization by a Commensal of the Human Microbiota
<li>A Packing Mechanism for Nucleosome Organization Reconstituted Across a Eukaryotic Genome
<li>Structures of the Bacterial Ribosome in Classical and Hybrid States of tRNA Binding
</ul>

and did the same for the <a href="http://www.cell.com/issue?pii=S0092-8674(11)X0010-7">current issue</a> of <i>Cell</i>:


<ul>
<li>Mapping the NPHP-JBTS-MKS Protein Network Reveals Ciliopathy Disease Genes and Pathways
<li>Double-Strand Break Repair-Independent Role for BRCA2 in
  Blocking Stalled Replication Fork Degradation by MRE11
<li>Establishment and Maintenance of Alternative Chromatin States at a Multicopy Gene Locus
<li>An Epigenetic Signature for Monoallelic Olfactory Receptor Expression
<li>Distinct p53 Transcriptional Programs Dictate Acute DNA-Damage Responses and Tumor Suppression
<li>An ADIOL-ER&beta;-CtBP Transrepression Pathway Negatively Regulates Microglia-Mediated Inflammation
<li>A Hormone-Dependent Module Regulating Energy Balance
<li>Class IIa Histone Deacetylases Are Hormone-Activated
  Regulators of FOXO and Mammalian Glucose Homeostasis
</ul>

and for the <a
href="http://nobelprize.org/nobel_prizes/lists/year/">2010 Nobel
Prizes</a> in science:
<ul>
<li>Physics: <i>for groundbreaking experiments regarding
the two-dimensional material graphene</i>
<li>Chemistry: <i>for palladium-catalyzed cross couplings in organic
 synthesis</i>
<li>Physiology or Medicine: <i>for the development of in vitro
 fertilization</i>
</ul>

My conclusion is that 100% of these articles and awards are more about
"accurately modeling the world" than they are about "providing
insight," although they all have some
theoretical insight component as well. I recognize that judging one
way or the other is a difficult ill-defined task, and that you
shouldn't automatically accept my judgements, because I may have an inherent bias.  (I
was considering running an experiment on Mechanical Turk to get an
unbiased answer, but those familiar with Mechanical Turk told me these
questions are probably too hard for the average Turker.  So you the reader can do your own
experiment and see if you agree.)

<h2>What doesn't Chomsky like about statistical models?</h2>

<p>I said that statistical models are sometimes confused with
probabilistic models; let's first consider the extent to which Chomsky's
objections are actually about probabilistic models. In 1969 he
famously <a href="http://books.google.com/books?id=iaXVXYDQN1oC&pg=PA296&dq=%22probability+of+a+sentence%22+%22entirely+useless%22&hl=en&ei=_s7eTc_5DrTciALcsvnlCg&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDoQ6AEwAw#v=onepage&q=%22probability%20of%20a%20sentence%22%20%22entirely%20useless%22&f=false">wrote</a>:
<blockquote>
But it must be recognized that the notion of "probability of a sentence" is an entirely useless one, under any known interpretation of this term.
</blockquote>

<p>His main argument being that, under any interpretation known to him,
 the probability of a novel sentence must be zero, and since novel sentences
 are in fact generated all the time, there is a contradiction.  The resolution of
 this contradiction is of course that it is not necessary to assign a
 probability of zero to a novel sentence; in fact, with current
 probabilistic models it is standard practice to do smoothing and assign a non-zero
 probability to novel occurrences. So this criticism is invalid, but was
very influential for decades.
Previously, in
<a
href="http://books.google.com/books?id=SNeHkMXHcd8C&printsec=frontcover&dq=syntactic+structures+chomsky&hl=en&src=bmrr&ei=1WvcTa3UDeLQiAK6-4zpDw&sa=X&oi=book_result&ct=book-thumbnail&resnum=1&ved=0CD4Q6wEwAA#v=onepage&q=probabilistic&f=false"><i>Syntactic
Structures</i></a> (1957) Chomsky wrote:

<blockquote>
I think we are forced to conclude that ... probabilistic models give no
particular insight into some of the basic problems of syntactic structure.
</blockquote>

In the footnote to this conclusion he considers the possibility
of a useful probabilistic/statistical model, saying "I would certainly
not care to argue that ... is unthinkable, but I know of no suggestion
to this effect that does not have obvious flaws." The main "obvious flaw" is this: 
<a href="https://preview.redd.it/ib8ws5sfode41.jpg?width=960&crop=smart&auto=webp&v=enabled&s=6674d246edb510da0c5e3c3c4380f640a7029194">Consider:</a>
<ol>
 <li> <b>I</b> never, ever, ever, ever, ... <b>fiddle</b> around in any way with electrical equipment.
 <li> <b>She</b> never, ever, ever, ever, ... <b>fiddles</b> around in any way with electrical equipment.
 <li> * <b>I</b> never, ever, ever, ever, ... <b>fiddles</b> around in any way with electrical equipment.
 <li> * <b>She</b> never, ever, ever, ever, ... <b>fiddle</b> around in any way with electrical equipment.
</ol>
No matter how many repetitions of "ever" you insert, sentences 1 and 2
are grammatical and 3 and 4 are ungrammatical. A probabilistic
Markov-chain model
with <i>n</i> states can never make the necessary distinction (between
1 or 2 versus 3 or 4) when there are more than <i>n</i> copies of "ever."
Therefore, a probabilistic Markov-chain model cannot handle all of English.

<p>This criticism is correct, but it is a criticism only of Markov-chain
models, not of probabilistic models (or
trained models) in general. Since 1957 we have seen many
types of probabilistic language models beyond the Markov-chain word
models.   Examples 1-4 above can in fact be distinguished with a
finite-state model that is not a chain, but other examples require
more sophisticated models. The best studied is probabilistic context-free grammar (PCFG),
which operates over trees, categories of words, and individual lexical
items, and has none of the
restrictions of finite-state models. We find that PCFGs are
state-of-the-art for parsing performance and are easier to learn from
data than nonprobabilistic categorical context-free grammars. Other types of probabilistic
models cover semantic and discourse structures. 
<p>
Every probabilistic model
is a superset of a deterministic model (because the deterministic model could be seen as a
probabilistic model where the probabilities are restricted to be 0 or
1), so any valid criticism of probabilistic models would have to be
because they are too expressive, not because they are not expressive
enough.

<div style="float:right; margin:8px; text-align:center">
  <a href="http://www.wmjasco.com/limits/1987.html"><img
  src="http://3.bp.blogspot.com/_kzW2Xjzh-Wc/SRtrl7AqdOI/AAAAAAAAAXQ/nRLtfprd-kQ/s400/Colorless+green+ideas.jpg"
  title="Colorless green ideas sleep furiously" width=106></a>
</div>
<p>In <i>Syntactic Structures</i>, Chomsky introduces a now-famous
example that is another criticism of finite-state probabilistic models:
<blockquote>
Neither (a) 'colorless green ideas sleep furiously' nor (b) 'furiously
sleep ideas green colorless', nor any of their parts, has ever occurred
in the past linguistic experience of an English speaker. But (a) is
grammatical, while (b) is not.
</blockquote>
Chomsky appears to be correct that neither sentence appeared in the
published literature before 1955. I'm not sure what he meant by "any
of their parts," but certainly every two-word part had occurred, for example:
<ul>
  <li>"It is neutral green, <b>colorless green</b>, like the glaucous water lying in
a cellar."
<a
href="http://books.google.com/books?id=QLUaAAAAMAAJ&q=%22colorless+green%22&dq=%22colorless+green%22&hl=en&ei=iZjcTZj1FMHhiAL9yqUF&sa=X&oi=book_result&ct=result&resnum=7&ved=0CE0Q6AEwBg">The
Paris we remember</a>, Elisabeth Finley Thomas (1942).
  <li>"To specify those <b>green ideas</b> is hardly necessary, but you may
observe Mr. [D. H.] Lawrence in the role of the satiated aesthete."
<a
href="http://books.google.com/books?id=FodMAAAAYAAJ&pg=PA184&dq=%22green+ideas%22&hl=en&ei=vZncTYaLHsbYiAKz2vDoDw&sa=X&oi=book_result&ct=result&resnum=3&ved=0CEIQ6AEwAjgK">The
New Republic: Volume 29</a> p. 184, William White (1922).

<li><b>"Ideas sleep</b> in books." <a
 href="http://books.google.com/books?id=fTciAQAAIAAJ&pg=PA96&dq=%22ideas+sleep%22&hl=en&ei=bZrcTePRBeHmiAKp2fHoDw&sa=X&oi=book_result&ct=result&resnum=3&sqi=2&ved=0CDoQ6AEwAg#v=onepage&q=%22ideas%20sleep%22&f=false">Current
Opinion: Volume 52</a>, (1912).
 </ul>

<p>But regardless of what is meant by "part," a
statistically-trained finite-state model <i>can</i> in fact distinguish
between these two sentences. Pereira (2001) <a
 href="http://www.cis.upenn.edu/~pereira/papers/rsoc.pdf">showed</a>
that such a model, augmented with word categories and trained by
 expectation maximization on newspaper text, computes that  (a) is 200,000 times more
probable than (b). To prove that this was not the result of Chomsky's
 sentence itself sneaking into newspaper text, I repeated the
 experiment, using a much cruder model
with Laplacian smoothing and no categories, trained over the <a
 href="http://ngrams.googlelabs.com/graph?content=colorless+green&year_start=1800&year_end=1954&corpus=0&smoothing=3">Google
 Book corpus</a> from 1800 to 1954, and found that (a) is about 10,000
 times more probable. If we had a probabilistic model over trees as
 well as word sequences, we could perhaps do an even better job of
 computing degree of grammaticality. 

<p>Furthermore, the statistical models are capable of delivering the
judgment that both
sentences are <i>extremely</i> improbable, when compared to, say,
"Effective green products sell well." Chomsky's theory, being categorical,
cannot make this distinction; all it can distinguish is
grammatical/ungrammatical.

<p>Another part of Chomsky's objection is "we cannot seriously propose
that a child learns the values of 10<sup>9</sup> parameters in a
childhood lasting only 10<sup>8</sup> seconds."  (Note that modern
 models are much larger than the
 10<sup>9</sup> parameters that were contemplated in the 1960s.) But of course
 nobody is proposing that these parameters are learned one-by-one; the
 right way to do learning is to set
 large swaths of near-zero parameters simultaneously with a
 smoothing or regularization procedure, and update the high-probability
 parameters continuously as observations comes in.  Nobody is
 suggesting that Markov models by themselves are a serious model of human language
 performance. But I (and others) suggest that probabilistic, trained
models are a better model of human language performance than are
categorical, untrained models.  And yes, it seems clear that an adult
speaker of English does know billions of language facts (a speaker knows many
facts about the appropriate uses of words in different contexts, such as that one
says "the big game" rather than "the large game" when talking about an
important football game). These facts must somehow be encoded in the
brain. 

<p>It seems clear that probabilistic models are better for judging the
likelihood of a sentence, or its degree of sensibility. But even if
you are not interested in these factors and are only interested in the
grammaticality of sentences, it still seems that probabilistic models
do a better job at describing the linguistic facts.  The
<i>mathematical</i> theory of <a
 href="http://en.wikipedia.org/wiki/Formal_language">formal
languages</a> defines a language as a set of sentences.  That is,
every sentence is either grammatical or ungrammatical; there is no
need for probability in this framework.  But natural languages are not
like that. A <i>scientific</i> theory of natural languages must
account for the many phrases and sentences which leave a native
speaker uncertain about their grammaticality (see Chris Manning's <a
 href="http://nlp.stanford.edu/~manning/papers/probsyntax.pdf">article</a>
and its discussion of the phrase "<a
 href="http://ngrams.googlelabs.com/graph?content=as+least+as&year_start=1800&year_end=2000&corpus=0&smoothing=3">as
least as</a>"), and there are phrases which some speakers find
perfectly grammatical, others perfectly ungrammatical, and still
others will flip-flop from one occasion to the next. Finally, there
are usages which are rare in a language, but cannot be dismissed if
one is concerned with actual data.  For example, the verb <i>quake</i>
is listed as intransitive in dictionaries, meaning that (1) below is
grammatical, and (2) is not, according to a categorical theory of
grammar.  <ol> <li> The earth quaked.  <li> ? It quaked her bowels.
</ol> But (2) <a
 href="http://books.google.com/books?id=nwmgdvN_akAC&pg=PA107&lpg=PA107&dq=%22quaked+her%22&source=bl&ots=9M4UljFMWl&sig=2BHbXzUY8dmZXEDXkoflNmrf_Zg&hl=en&ei=ItXeTYC0Js3SiAKnmon3Cg&sa=X&oi=book_result&ct=result&resnum=1&ved=0CB4Q6AEwADgK#v=onepage&q=%22quaked%20her%22&f=false">actually</a>
<a
 href="http://www.lexchecker.org/showhygramexample.php?lemmapos_id=1108&hygram_len=3&target_position=0&max_ignore_rate=50&hygram_id=149">appears</a>
as a sentence of English.  This poses a dilemma for the categorical
theory.  When (2) is observed we must either arbitrarily
dismiss it as an error that is outside the bounds of our model
(without any theoretical grounds for doing so), or we must
change the theory to allow (2), which often results in the acceptance
of a flood of
sentences that we would prefer to remain ungrammatical. As Edward
Sapir <a href="http://books.google.com/books?id=ofgrAAAAYAAJ&pg=PA39&dq=%22all+grammars+leak%22+sapir+1921&hl=en&ei=wefeTaXhKMTPiALfw6jWCg&sa=X&oi=book_result&ct=book-thumbnail&resnum=10&ved=0CFsQ6wEwCQ#v=onepage&q=all%20grammars%20leak&f=false">said</a> in 1921, "All grammars leak."  But in a probabilistic model there is
no difficulty; we can say that <i>quake</i> has a high probability of
being used intransitively, and a low probability of transitive use
(and we can, if we care, further describe those uses through
subcategorization).

<p>Steve Abney <a href="http://www.vinartus.net/spa/95c.pdf">points
  out</a> that probabilistic models are better suited for modeling
  language change.  He cites the example of a 15th century Englishman
  who goes to the pub every day and orders "Ale!"  Under a categorical model,
  you could reasonably expect that one day he would be served eel, because the <a href="http://en.wikipedia.org/wiki/Great_Vowel_Shift">great vowel shift</a>
  flipped a Boolean parameter in his mind a day before it flipped the
  parameter in the publican's.  In a probabilistic framework, there
  will be multiple parameters, perhaps with continuous values, and it
  is easy to see how the shift can take place gradually over two centuries.

<p>Thus it seems that grammaticality is not a categorical,
deterministic judgment but rather an inherently probabilistic
one. This becomes clear to anyone who spends time making <i>observations</i>
of a corpus of actual sentences, but can remain unknown to those who
think that the object of study is their own set of <i>intuitions</i> about
grammaticality.  Both observation and intuition have been used in the
history of science, so neither is "novel," but it is observation, not
intuition that is the dominant model for science.

<p>Now let's consider what I think is Chomsky's main point of
disagreement with statistical models:
the tension between "accurate description" and "insight." This is an old
distinction. Charles Darwin (biologist, 1809&ndash;1882) is best known for his
insightful theories but he stressed the importance of accurate
description, saying "False facts are highly injurious to the progress
of science, for they often endure long; but false views, if supported
by some evidence, do little harm, for every one takes a salutary
pleasure in proving their falseness." More recently, Richard Feynman
(physicist, 1918&ndash;1988) wrote "Physics can progress without the
proofs, but we can't go on without the facts."

<div style="float:right; margin:8px; text-align:center">
  <a href="http://en.wikipedia.org/wiki/Insect_collecting"><img
  src="http://upload.wikimedia.org/wikipedia/commons/archive/e/e8/20110128041836%21A_butterfly_collection.jpg"
  title="Butterfly Collection" width=106></a>
  <br>Butterflies
  </div>


<p>On the other side, Ernest Rutherford (physicist, 1871&ndash;1937)
disdained mere description, saying "All science is either physics or
stamp collecting."   Chomsky stands with him: "You can also collect
butterflies and make many observations. If you like butterflies,
that's fine; but such work must not be confounded with research, which
is concerned to discover explanatory principles."</p>

<p>Acknowledging both sides is Robert Millikan
(physicist, 1868&ndash;1953) who said in his Nobel acceptance speech
"Science walks forward on two feet, namely theory and experiment
... Sometimes it is one foot that is put forward first, sometimes the
other, but continuous progress is only made by the use of both."


<h2>The two cultures</h2>

  <div style="float:right; margin:8px; text-align:center">
    <a href="http://www.stat.berkeley.edu/~breiman/"><img
  src="https://upload.wikimedia.org/wikipedia/en/4/4d/Leo_Breiman.jpg"
  title="Leo Breiman" width=106></a>
  <br>Leo Breiman
</div>

<p>After all those distinguished scientists have weighed in, I think
the most relevant contribution to the current discussion is the 2001 paper by
Leo Breiman (statistician, 1928&ndash;2005), <a
 href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf">Statistical
Modeling: The Two Cultures</a>. In this paper Breiman, alluding to
C. P. Snow, describes two cultures:

<p>First the <b>data modeling culture</b> (to which, Breiman
estimates, 98% of statisticians subscribe) holds that nature can be
described as a black box that has a relatively simple underlying model
which maps from input variables to output variables (with perhaps some
random noise thrown in).  It is the job of the statistician to wisely
choose an underlying model that reflects the reality of nature, and
then use statistical data to estimate the parameters of the model.

<p>Second the <b>algorithmic modeling culture</b> (subscribed to by 2%
of statisticians and many researchers in biology, artificial
intelligence, and other fields that deal with complex phenomena),
which holds that nature's black box cannot necessarily be described by a simple
model.  Complex algorithmic approaches (such as support vector
machines or boosted decision trees or deep belief networks) are used to estimate the
function that maps from input to output variables, but we have no
expectation that the <i>form</i> of the function that emerges from
this complex algorithm reflects the true
underlying nature.

<p>It seems that the algorithmic modeling culture is what Chomsky is
objecting to most vigorously. It is not just that the models are
statistical (or probabilistic), it is that they produce a form that,
while accurately modeling reality, is not easily interpretable by
humans, and makes no claim to correspond to the generative process
used by nature. In other words, algorithmic modeling describes what
<i>does</i> happen, but it doesn't answer the question of <i>why</i>.

<p>Breiman's article explains his objections to the first
culture, data modeling.  Basically, the conclusions made by data
modeling are about the model, not about nature. (Aside: I remember in
2000 hearing <a
href="http://www.nasa.gov/home/hqnews/2002/02-072.txt">James
Martin</a>, the leader of the Viking missions to Mars, saying that his
job as a spacecraft engineer was not to land on Mars, but to land on
the model of Mars provided by the geologists.) The problem is, if the
model does not emulate nature well, then the conclusions may be
wrong. For example, linear regression is one of the most powerful
tools in the statistician's toolbox.  Therefore, many analyses start
out with "Assume the data are generated by a linear model..." and lack
sufficient analysis of what happens if the data are not in fact
generated that way. In addition, for complex problems there are
usually many alternative good models, each with very similar measures of goodness
of fit.  How is the data modeler to choose between them? Something has
to give.  Breiman is inviting us to give up on the idea that we can
uniquely model the true underlying <i>form</i> of nature's function
from inputs to outputs.  Instead he asks us to be satisfied with a
function that accounts for the observed data well, and generalizes to
new, previously unseen data well, but may be expressed in a complex
mathematical form that may bear no relation to the "true" function's
form (if such a true function even exists). Chomsky takes the opposite
approach: he prefers to keep a simple, elegant model, and give up on
the idea that the model will represent the data well. Instead, he
declares that what he calls <i>performance</i> data&mdash;what people
actually do&mdash;is off limits to linguistics; what really matters is
<i>competence</i>&mdash;what he imagines that they should do.

<div style="float:right; margin:8px; text-align:center">
  <img
  src="http://deerhart.files.wordpress.com/2011/02/bill-oreilly.jpg"
  title="Can't explain that." width=106>
  <br>Bill O'Reilly
  <br>
  <a href="http://en.wikipedia.org/wiki/Pierre-Simon_Laplace"><img
  src="http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Pierre-Simon_Laplace.jpg/90px-Pierre-Simon_Laplace.jpg"
  title="Pierre-Simon Laplace" width=110></a>
  <br>Laplace
</div>

<p>In January of 2011, television personality Bill O'Reilly weighed in
on more than one culture war with his statement "<i><a
 href="https://www.youtube.com/watch?v=wb3AFMe2OQY">tide goes in, tide
goes out. Never a miscommunication. You can't explain that</a></i>,"
which he proposed as an argument for the existence of God. O'Reilly
was ridiculed by his detractors for not knowing that tides can be
readily and concisely explained by a system of partial differential equations
describing the gravitational interaction of sun, earth, and moon (a
fact that was first <a href="http://en.wikipedia.org/wiki/Laplace%27s_tidal_equations#Laplace.27s_tidal_equations">worked out</a> by Laplace in 1776 and has been
considerably refined since; when asked by Napoleon why the creator did
not enter into his calculations, Laplace said "I had no need of that
hypothesis."). (O'Reilly also seems not to know about Deimos and
Phobos (two of my favorite moons in the entire solar system, along with
Europa, Io, and Titan), nor that Mars and Venus orbit the sun, nor that the reason Venus has no
moons is because it is so close to the sun that there is scant room for a
stable lunar orbit.) 

<p>But O'Reilly realizes that it doesn't matter what
his detractors think of his astronomical ignorance, because his
supporters think he has gotten exactly to the key issue: <i>why?</i>
He doesn't care <i>how</i> the tides work, tell him <i>why</i> they
work. <i>Why</i> is the moon at the right distance to provide a gentle tide,
and exert a stabilizing effect on earth's axis of rotation, thus
protecting life here? <i>Why</i> does gravity work the way it does?  <i>Why</i> does
anything at all exist rather than not exist? O'Reilly is correct that
these <i>why</i> questions can only be addressed by mythmaking, religion or
philosophy, not by science.

<p>Chomsky has a philosophy based on the
idea that we should focus on the deep <i>whys</i> and that mere
explanations of reality don't matter. In this, Chomsky is in complete
agreement with O'Reilly. (I recognize that the previous sentence would
have an extremely low probability in a probabilistic model trained
on a newspaper or TV corpus.)
Chomsky believes a theory of language should be simple and understandable,
like a linear regression model where we know the underlying process is
a straight line, and all we have to do is estimate the slope and
intercept.

<p>For example, consider the notion of a <a
href="http://en.wikipedia.org/wiki/Pro-drop_language">pro-drop
language</a> from Chomsky's <a
href="http://books.google.com/books?id=l08tpkOOdNQC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q=pro-drop&f=false">Lectures
on Government and Binding</a> (1981). In English we say, for example,
"I'm hungry," expressing the pronoun "I".  But in Spanish, one
expresses the same
thought with
"Tengo hambre" (literally "have hunger"), dropping the
pronoun "Yo". Chomsky's theory is that there is a "pro-drop parameter"
which is "true" in Spanish and "false" in English, and that once we
discover the small set of parameters that describe all languages, and
the values of those parameters for each language, we will have
achieved true understanding.



<div style="float:right; margin:8px; text-align:center">
  <a href="http://www.danacarvey.net/"><img
  src="http://3.bp.blogspot.com/_1OFe6OW0NaI/RZMEX1KaEMI/AAAAAAAAAHk/8dr866Pg6sI/s320/Dana+Carvey+as+Bush.jpg"
  title="Not gonna do it. Wouldn't be prudent." width=106></a>
  <br>Dana Carvey
</div>

<p>The problem is that reality is messier than this theory.  Here are
some dropped pronouns in English:
<ul>
<li> "Not gonna do it. Wouldn't be prudent." (Dana Carvey, <a
href="http://snltranscripts.jt.org/94/94dmono.phtml">impersonating
George H. W. Bush</a>)

<li>"Thinks he can outsmart us, does he?" (Evelyn Waugh, <a href="http://brothersjudd.com/index.cfm/fuseaction/reviews.detail/book_id/891">The Loved One</a>)

  <li>"Likes to fight, does he?" (S.M. Stirling, <a
  href="http://books.google.com/books?id=zOLTi9LRG78C&printsec=frontcover&dq=the+sunrise+lands&hl=en&ei=PlDbTfTPBpL0swOR_4W_Dg&sa=X&oi=book_result&ct=result&resnum=1&ved=0CCoQ6AEwAA#v=onepage&q=likes%20to%20fight%20does%20he&f=false">The
  Sunrise Lands</a>)

<li>"Thinks he's all that." (Kate Brian, <a
href="http://books.google.com/books?id=ERoDoSk7FN4C&pg=PT190&dq=%22thinks+he's+all+that%22&hl=en&ei=djzWTaKDK4zGsAPLxYWxBw&sa=X&oi=book_result&ct=result&resnum=1&ved=0CCkQ6AEwADgK#v=onepage&q=%22thinks%20he's%20all%20that%22&f=false">Lucky
T</a>)

  <li> "Go for a walk?" (countless dog owners)
  <li> "Gotcha!"  "Found it!" "Looks good to me!" (common expressions)
</ul>

Linguists can argue over the interpretation of these facts for hours
on end, but the diversity of language seems to be much more complex
than a single Boolean value for a pro-drop parameter. We shouldn't
accept a theoretical framework that places a priority on making the
model simple over making it accurately reflect reality.

<p>From the beginning, Chomsky has focused on the <i>generative</i>
side of language. From this side, it is reasonable to tell a
non-probabilistic story: I <i>know</i> definitively the idea I want to
express&mdash;I'm starting from a single semantic form&mdash;thus all I
have to do is choose the words to say it; why can't that be a
deterministic, categorical process? If Chomsky had focused on the
other side, <i>interpretation</i>, as Claude Shannon did, he may have changed
his tune.  In interpretation (such as speech recognition) the listener
receives a noisy, ambiguous signal and needs to decide which of many
possible intended messages is most likely.  Thus, it is obvious that
this is inherently a probabilistic problem, as was recognized early on
by all researchers in speech recognition, and by scientists in other
fields that do interpretation: the astronomer Laplace said in 1819
"Probability theory is nothing more than common sense reduced to
calculation," and the physicist James Maxwell said in 1850 "The true
logic for this world is the calculus of Probabilities, which takes
account of the magnitude of the probability which is, or ought to be,
in a reasonable man's mind."

<p>Finally, one more reason why Chomsky dislikes statistical models is
that they tend to make linguistics an empirical science (a science
about how people actually use language) rather than a mathematical
science (an investigation of the mathematical properties of <i>models</i> of
formal language, not of language itself).  Chomsky prefers the later, as evidenced by his statement
in <a
 href="http://books.google.com/books?id=u0ksbFqagU8C&lpg=PP1&dq=aspects%20of%20the%20theory%20of%20syntax&pg=PA4#v=snippet&q=%22observed%20use%20of%20language%22&f=false"><i>Aspects of the Theory of
Syntax</i></a> (1965):
<blockquote>
Linguistic theory is mentalistic, since it is concerned with
discovering a mental reality underlying actual behavior. Observed use of language ... may
provide evidence ... but surely cannot constitute the subject-matter
of linguistics, if this is to be a serious discipline.
</blockquote>
I can't imagine Laplace saying that observations of the planets
cannot constitute the subject-matter of orbital mechanics, or Maxwell
saying that observations of electrical charge cannot constitute the
subject-matter of electromagnetism. It is true that physics considers
idealizations that are abstractions from the messy real world.  For
example, a class of mechanics problems ignores friction.  But that
doesn't mean that friction is not considered part of the
subject-matter of physics. 

<p><div style="float:right; margin:8px; text-align:center">
  <a href="http://en.wikipedia.org/wiki/Allegory_of_the_Cave"><img
  src="http://bluelungimagaa.files.wordpress.com/2010/03/cave.gif?w=200&h=150"
  title="Plato's allegory of the cave" width=150></a>
  <br>Plato's cave
</div>

So how could Chomsky say that observations of language cannot be
the subject-matter of linguistics? It seems to come from his viewpoint
as a <a href="http://en.wikipedia.org/wiki/Platonist">Platonist</a>
and a <a href="http://en.wikipedia.org/wiki/Rationalism">Rationalist</a>
and perhaps a bit of a <a href="http://en.wikipedia.org/wiki/Mysticism">Mystic</a>.  As
in Plato's <a
 href="http://en.wikipedia.org/wiki/Allegory_of_the_Cave">allegory of
the cave</a>, Chomsky thinks we should focus on the ideal, abstract
forms that underlie language, not on the superficial manifestations of
language that happen to be perceivable in the real world. That is why
he is not interested in language performance. But Chomsky, like Plato,
has to answer where these ideal forms come from.  Chomsky (1991) shows
that he is happy with a Mystical answer, although he shifts vocabulary
from "soul" to "biological endowment."

<blockquote> Plato's answer was that the knowledge is 'remembered'
from an earlier existence. The answer calls for a mechanism: perhaps
the immortal soul ... rephrasing Plato's answer in terms more
congenial to us today, we will say that the basic properties of
cognitive systems are innate to the mind, part of human biological
endowment. </blockquote>

<div style="float:right; margin:8px; text-align:center">
  <a href="http://en.wikipedia.org/wiki/File:Lascaux2.jpg"><img
  src="http://upload.wikimedia.org/wikipedia/commons/thumb/0/07/Lascaux2.jpg/120px-Lascaux2.jpg"
  title="Lascaux Horse" width=106></a>
  <br>Lascaux Horse
</div>

<p> It was reasonable for Plato to think that the ideal of, say, a
horse, was more important than any individual horse we can perceive in
the world.  In 400BC, species were thought to be eternal and
unchanging. We now know that is not true; that the horses on another
cave wall&mdash;in Lascaux&mdash;are now extinct, and that current
horses continue to evolve slowly over time.  Thus there is no such
thing as a single ideal eternal "horse" form.

<p> We also now know that language is like that as well: languages are
complex, random, contingent biological processes that are subject to
the whims of evolution and cultural change. What constitutes a
language is not an eternal ideal form, represented by the settings of
a small number of parameters, but rather is the contingent outcome of
complex processes.  Since they are contingent, it seems they can
only be analyzed with probabilistic models. Since people have to
continually understand the uncertain, ambiguous, noisy speech of
others, it seems they must be using something like probabilistic reasoning.  Chomsky for some reason
wants to avoid this, and therefore he must declare the actual facts of
language use out of bounds and declare that true linguistics only
exists in the mathematical realm,
where he can impose the formalism he wants.  Then, to get
language from this abstract, eternal, mathematical realm into the
heads of people, he must fabricate a mystical facility that is exactly
tuned to the eternal realm. This may be very
interesting from a mathematical point of view, but it misses the point
about what language is, and how it works.


<h2>Thanks</h2>

Thanks to Ann Farmer, Fernando Pereira, Dan Jurafsky, Hal Varian, and others for comments
and suggestions on this essay.

<h2>Annotated Bibliography</h2>
<ol>
  <li>Abney, Steve (1996) <a
  href="http://www.vinartus.net/spa/95c.pdf">Statistical Methods and
  Linguistics</a>, in Klavans and Resnik (eds.) <i>The Balancing Act: Combining Symbolic and
  Statistical Approaches to Language</i>, MIT Press.
  <blockquote><i>An excellent overall introduction to the statistical
  approach to language processing, and covers some ground that is not
  addressed often, such as language change and individual differences.</i></blockquote>

  <li>Breiman, Leo (2001) <a
  href="http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1009213726">Statistical Modeling: The
  Two Cultures</a>, <i>Statistical Science</i>, Vol. 16, No. 3, 199-231.
  <blockquote><i>Breiman does a great job of describing the two
  approaches, explaining the benefits of his approach, and defending
  his points in the vary interesting commentary with eminent
  statisticians: Cox, Efron, Hoadley, and Parzen.</i></blockquote>
  
  <li>Chomsky, Noam (1956) <a
  href="http://www.chomsky.info/articles/195609--.pdf">Three Models
  for the Description of Language</a>, <i>IRE Transactions on
  Information theory</i> (2), pp. 113-124.
  <blockquote><i>Compares finite state, phrase structure, and
  transformational grammars. Introduces "colorless green ideas sleep
  furiously."</i></blockquote>

    <li>Chomsky, Noam (1967) <a
  href="http://books.google.com/books?id=SNeHkMXHcd8C&printsec=frontcover&dq=syntactic+structures+chomsky&hl=en&src=bmrr&ei=1WvcTa3UDeLQiAK6-4zpDw&sa=X&oi=book_result&ct=book-thumbnail&resnum=1&ved=0CD4Q6wEwAA#v=onepage&q=probabilistic&f=false">Syntactic Structures</a>, Mouton.
  <blockquote><i>A book-length exposition of Chomsky's theory that was
  the leading exposition of linguistics for a decade.  Claims that
  probabilistic models give no insight into syntax.</i></blockquote>
  
  <li>Chomsky, Noam (1969) <a
  href="http://books.google.com/books?id=iaXVXYDQN1oC&pg=PA296&dq=%22probability+of+a+sentence%22+%22entirely+useless%22&hl=en&ei=_s7eTc_5DrTciALcsvnlCg&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDoQ6AEwAw#v=onepage&q=%22probability%20of%20a%20sentence%22%20%22entirely%20useless%22&f=false">Some Empirical
  Assumptions in Modern Philosophy of Language</a>, in <i>Philosophy,
  Science and Method: Essays in Honor or Ernest Nagel</i>,
  St. Martin's Press.
  <blockquote><i>Claims that the notion "probability of a sentence" is
  an entirely useless notion.</i></blockquote>

  <li>Chomsky, Noam (1981) <a
  href="http://books.google.com/books?id=l08tpkOOdNQC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q=pro-drop&f=false">Lectures
  on government and binding</a>, de Gruyer.
  <blockquote><i>A revision of Chomsky's theory; this version
  introduces Universal Grammar.  We cite it for the coverage of
  parameters such as pro-drop.</i></blockquote>

  <li>Chomsky, Noam (1991) <a
  href="http://www.cpgb.org.uk/article.php?article_id=1004261#23">Linguistics
  and adjacent fields: a personal view</a>, in Kasher (ed.), <i>A
  Chomskyan Turn</i>, Oxford.
  <blockquote><i>I found the Plato quotes in <a
  href="http://www.cpgb.org.uk/article.php?article_id=1004261#23">this</a>
  article, published by the Communist Party of Great Britain, and
  apparently published by someone with no linguistics training
  whatsoever, but with a political agenda.</i></blockquote>

  <li>Gold, E. M. (1967) <a
  href="http://en.wikipedia.org/wiki/Language_identification_in_the_limit">Language
  Identification in the Limit</a>, <i>Information and Control</i>,
  Vol. 10, No. 5, pp. 447-474.
  <blockquote><i>Gold proved a result in formal language theory that
  we can state (with some artistic license) as this: imagine a game
  between two players, guesser and chooser.  Chooser says to
  guesser, "Here is an infinite number of languages. I'm going to
  choose one of them, and start reading sentences to you that come
  from that language. On your N-th birthday there will be a True-False
  quiz where I give you 100 sentences you haven't heard yet, and you
  have to say whether they come from the language or not."  There are
  some limits on what the infinite set looks like and on how 
  the chooser can pick sentences (he can be deliberately
  tricky, but he can't just repeat the same sentence over and over,
  for example). Gold's
  result is that if the infinite set of languages are all generated by
  context-free grammars then there is no strategy for guesser that
  guarantees she gets 100% correct every time, no matter what N you
  choose for the birthday. This result was
  taken by Chomsky and others to mean that it is impossible for
  children to learn human languages without having an innate "language
  organ."  As <a href="http://psyling.psy.cmu.edu/papers/years/2004/logical/gold-johnson.pdf ">Johnson (2004)</a> and others show, this was an invalid
  conclusion; the task of getting 100% on the quiz (which Gold called
  <i>language identification</i>) 
  really has nothing in common with the task of <i>language acquisition</i>
  performed by children, so Gold's Theorem has no
  relevance.</i></blockquote>

  <li>Horning, J. J. (1969) <a href="http://portal.acm.org/citation.cfm?id=905718&coll=DL&dl=GUIDE&CFID=23829333&CFTOKEN=19917759">A study of grammatical
  inference</a>, Ph.D. thesis, Stanford Univ.
  <blockquote><i>Where Gold found a negative result&mdash;that
  context-free languages were not identifiable from examples, Horning found a positive result&mdash;that probabilistic
  context-free languages are identifiable (to within an arbitrarily
  small level of error).  Nobody doubts that
  humans have unique innate capabilities for understanding language
  (although it is unknown to what extent these capabilities are
  specific to language and to what extent they are general cognitive
  abilities related to sequencing and forming abstractions). But
  Horning proved in 1969 that Gold cannot be used as a convincing
  argument for an innate language organ that specifies all of language
  except for the setting of a few parameters.</i></blockquote>

  <li>Johnson, Kent (2004) <a
  href="http://psyling.psy.cmu.edu/papers/years/2004/logical/gold-johnson.pdf">Gold's
  Theorem and cognitive science</a>, <i>Philosophy of Science</i>,
  Vol. 71, pp. 571-592.
  <blockquote><i>The best article I've seen on what Gold's Theorem
  actually says and what has been claimed about it (correctly and
  incorrectly).  Concludes that Gold has something to say about formal
  languages, but nothing about child language
  acquisition.</i></blockquote>

  <li>Lappin, Shalom and Shieber, Stuart M. (2007) <a
  href="http://www.dcs.kcl.ac.uk/staff/lappin/papers/lappin-shieber_learning07.pdf">Machine
  learning theory and practice as a source of insight into universal
  grammar.</a>, <i>Journal of Linguistics</i>, Vol. 43, No. 2,
  pp. 393-427.
<blockquote><i>An excellent article discussing the poverty of the
  stimulus, the fact that all models have bias,  the difference
  between supervised and unsupervised learning, and modern (PAC or VC)
  learning theory. It provides
  alternatives to the model of Universal Grammar consisting of a fixed
  set of binary parameters.</i></blockquote>

  <li>Manning, Christopher (2002) <a
  href="http://nlp.stanford.edu/~manning/papers/probsyntax.pdf">Probabilistic
  Syntax</a>, in Bod, Hay, and Jannedy (eds.), <i>Probabilistic
  Linguistics</i>, MIT Press.
  <blockquote><i>A compelling introduction to probabilistic syntax,
  and how it is a better model for linguistic facts than categorical
  syntax. Covers "the joys and perils of corpus
  linguistics."</i></blockquote>

  <li>Norvig, Peter (2007) <a
  href="http://norvig.com/spell-correct.html">How to Write a Spelling
  Corrector</a>, unpublished web page.
  <blockquote><i>Shows working code to implement a probabilistic,
  statistical spelling correction algorithm.</i></blockquote>

  <li>Norvig, Peter (2009) <a href="http://norvig.com/ngrams/">Natural
  Language Corpus Data</a>, in Seagran and Hammerbacher (eds.),
  <i>Beautiful Data</i>, O'Reilly.
<blockquote><i>Expands on the essay above; shows how to implement
  three tasks: text segmentation, cryptographic decoding, and spelling
  correction (in a slightly more complete form than the previous essay).</i></blockquote>

  <li>Pereira, Fernando (2002) <a
  href="http://www.cis.upenn.edu/~pereira/papers/rsoc.pdf">Formal
  grammar and information theory: together again?</a>, in Nevin and
  Johnson (eds.), <i>The Legacy of Zellig Harris</i>, Benjamins.
  <blockquote><i>When I set out to write the page you are reading now,
  I was concentrating on the events that took place in Cambridge,
  Mass., 4800 km from home.  After doing some research I was surprised
  to learn that the authors of
  two of the three best articles on this subject sit within a total of
  10 meters from my desk: Fernando Pereira and Chris Manning. (The
  third, Steve Abney, sits 3700 km away.) But perhaps I shouldn't have
  been surprised. I remember giving a talk at ACL on the corpus-based
  language models used at Google, and having Fernando, then a
  professor at U. Penn., comment "I feel like I'm a particle physicist
  and you've got the only super-collider."  A few years later he moved
  to Google. Fernando is also famous for his quote "The older I get, the
  further down the Chomsky Hierarchy I go." His article here covers
  some of the same ground as mine, but he goes farther in explaining
  the range of probabilistic models available and how they are useful.</i></blockquote>

  <li>Plato (c. 380BC) <a
  href="http://en.wikipedia.org/wiki/Allegory_of_the_Cave">The
  Republic</a>.
  <blockquote><i>Cited here for the allegory of the cave.</i></blockquote>

  <li>Shannon, C.E. (1948) <a
  href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">A
  Mathematical Theory of Communication</a>, <i>The Bell System
  Technical Journal</i>, Vol. 27, pp. 379-423.
   <blockquote><i>An enormously influential article that started the
  field of information theory and introduced the term "bit" and the noisy
  channel model, demonstrated successive n-gram approximations of
  English, described Markov models of language, defined entropy with
  respect to these models, and enabled the growth of the
  telecommunications industry.</i></blockquote>
</ol>
<p><hr>
<address></address>
<a href="http://norvig.com"><i>Peter Norvig</i></a>
</body>


<p><hr>
<div id="disqus_thread"></div><script type="text/javascript" src="http://disqus.com/forums/norvig/embed.js"></script><noscript><a href="http://norvig.disqus.com/?url=ref">View the forum thread.</a></noscript>

<script type="text/javascript">
//<[CDATA[
(function() {
links = document.getElementsByTagName('a');
query = '?';
for(var i = 0; i < links.length; i++) {
if(links[i].href.indexOf('#disqus_thread') >= 0) {
query += 'url' + i + '=' + encodeURIComponent(links[i].href) + '&';
}
}
document.write('<script type="text/javascript" src="http://disqus.com/forums/norvig/get_num_replies.js' + query + '"></' + 'script>');
})();
//]]>
</script>




</html>





